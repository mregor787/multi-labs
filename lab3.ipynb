{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Лабораторная работа №3: Проведение исследований с решающим деревом"
      ],
      "metadata": {
        "id": "Egt_kt_r8fzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Выбор начальных условий"
      ],
      "metadata": {
        "id": "WhE-bvv18h5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Был проведен в ЛР №1."
      ],
      "metadata": {
        "id": "Xe2qqlF38lBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub scikit-learn numpy pandas matplotlib seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMAk18q_8ncf",
        "outputId": "71d82a33-d183-46ed-94b2-d97fc2632f91"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Fec-Ikhr8q6p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачивание Date Fruit Dataset\n",
        "date_fruit_path = kagglehub.dataset_download(\"muratkokludataset/date-fruit-datasets\")\n",
        "\n",
        "# Чтение Excel-файла\n",
        "date_fruit_data = pd.read_excel(f\"{date_fruit_path}/Date_Fruit_Datasets/Date_Fruit_Datasets.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8gIBw3S8riy",
        "outputId": "85dad018-5c00-4268-a00e-a00b4f9b73ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/muratkokludataset/date-fruit-datasets?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 408k/408k [00:00<00:00, 40.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачивание Concrete Compressive Strength\n",
        "concrete_strength_path = kagglehub.dataset_download(\"niteshyadav3103/concrete-compressive-strength\")\n",
        "\n",
        "# Чтение CSV-файла\n",
        "concrete_data = pd.read_csv(f\"{concrete_strength_path}/Concrete Compressive Strength.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atuQgtNp8uTa",
        "outputId": "f489e7af-5b56-4955-8de5-6ec143a5aa8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/niteshyadav3103/concrete-compressive-strength?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14.1k/14.1k [00:00<00:00, 12.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Создание бейзлайна и оценка качества"
      ],
      "metadata": {
        "id": "XcxMCjc58weS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, root_mean_squared_error, r2_score, make_scorer\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "MnmB0-2h80gb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделим датасет для классификации на обучающую и тестовую выборки"
      ],
      "metadata": {
        "id": "L60pHI9y83kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение на признаки и целевую переменную\n",
        "X_class = date_fruit_data.drop(columns=['Class'])\n",
        "y_class = date_fruit_data['Class']\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
        "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "# Преобразование целевой переменной\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_class = label_encoder.fit_transform(y_train_class)\n",
        "y_test_class = label_encoder.transform(y_test_class)"
      ],
      "metadata": {
        "id": "-qUKJlAs84HL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналогично разделим датасет для регрессии"
      ],
      "metadata": {
        "id": "coGM9KrL88cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение на признаки и целевую переменную\n",
        "X_reg = concrete_data.drop(columns=['Concrete compressive strength '])\n",
        "y_reg = concrete_data['Concrete compressive strength ']\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Abx_Yc_j8_3n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модели для классификации и регрессии из Sklearn и оценим их"
      ],
      "metadata": {
        "id": "EFGAyHru9CT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_classifier = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "tree_classifier.fit(X_train_class, y_train_class)\n",
        "\n",
        "y_pred_class = tree_classifier.predict(X_test_class)\n",
        "\n",
        "accuracy_tree = accuracy_score(y_test_class, y_pred_class)\n",
        "f1_tree = f1_score(y_test_class, y_pred_class, average='weighted')\n",
        "\n",
        "print(f\"Decision Tree Classifier - Accuracy: {accuracy_tree:.4f}, F1-Score: {f1_tree:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYzM-QgI9Dvk",
        "outputId": "e5bac8cf-fe5a-494f-815d-0eae0800ace5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier - Accuracy: 0.8056, F1-Score: 0.7969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree_regressor = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "tree_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "y_pred_reg = tree_regressor.predict(X_test_reg)\n",
        "\n",
        "rmse_tree = root_mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2_tree = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"Decision Tree Regressor - RMSE: {rmse_tree:.4f}, R²: {r2_tree:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCAxH72v-aca",
        "outputId": "b82b4023-5f96-44ef-8825-ff6200fff1fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor - RMSE: 9.5792, R²: 0.6439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Итак, точность для встроенной в Sklearn модели решающего дерева получилась приемлемой (80.5% точности для классификатора, 9.57 RMSE для регрессора). Попробуем улучшить бейзлайн."
      ],
      "metadata": {
        "id": "ArW8OJXY-g1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Улучшение бейзлайна"
      ],
      "metadata": {
        "id": "1Vsm81j1-wjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будем перебирать гиперпараметры решающего дерева, такие, как max_depth, min_samples_split, min_samples_leaf"
      ],
      "metadata": {
        "id": "iFSkymhC_Nw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Подбор гиперпараметров для DecisionTreeClassifier\n",
        "print(\"=== Decision Tree Classifier - Hyperparameter Tuning ===\")\n",
        "param_grid_classifier = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "grid_search_classifier = GridSearchCV(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    param_grid=param_grid_classifier,\n",
        "    scoring=make_scorer(f1_score, average='weighted'),\n",
        "    cv=5,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search_classifier.fit(X_train_class, y_train_class)\n",
        "\n",
        "# Лучшие параметры и результат\n",
        "best_params_classifier = grid_search_classifier.best_params_\n",
        "best_score_classifier = grid_search_classifier.best_score_\n",
        "\n",
        "print(f\"Лучшие параметры для классификатора: {best_params_classifier}\")\n",
        "print(f\"Лучший F1-Score на кросс-валидации: {best_score_classifier:.4f}\")\n",
        "\n",
        "# 2. Подбор гиперпараметров для DecisionTreeRegressor\n",
        "print(\"\\n=== Decision Tree Regressor - Hyperparameter Tuning ===\")\n",
        "param_grid_regressor = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "grid_search_regressor = GridSearchCV(\n",
        "    estimator=DecisionTreeRegressor(random_state=42),\n",
        "    param_grid=param_grid_regressor,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=5,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Лучшие параметры и результат\n",
        "best_params_regressor = grid_search_regressor.best_params_\n",
        "best_score_regressor = -grid_search_regressor.best_score_\n",
        "\n",
        "print(f\"Лучшие параметры для регрессора: {best_params_regressor}\")\n",
        "print(f\"Лучший RMSE на кросс-валидации: {best_score_regressor:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz9pDWwU-vXK",
        "outputId": "e1fb13af-6e42-4402-f3fb-afa91f7eec53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Decision Tree Classifier - Hyperparameter Tuning ===\n",
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "Лучшие параметры для классификатора: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
            "Лучший F1-Score на кросс-валидации: 0.8272\n",
            "\n",
            "=== Decision Tree Regressor - Hyperparameter Tuning ===\n",
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "Лучшие параметры для регрессора: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
            "Лучший RMSE на кросс-валидации: 7.1226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модели с лучшими гиперпараметрами"
      ],
      "metadata": {
        "id": "HDevF-JhAktZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Обучение DecisionTreeClassifier с лучшими параметрами\n",
        "print(\"\\n=== Обучение Decision Tree Classifier с лучшими параметрами ===\")\n",
        "best_tree_classifier = DecisionTreeClassifier(\n",
        "    max_depth=best_params_classifier['max_depth'],\n",
        "    min_samples_split=best_params_classifier['min_samples_split'],\n",
        "    min_samples_leaf=best_params_classifier['min_samples_leaf'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "best_tree_classifier.fit(X_train_class, y_train_class)\n",
        "\n",
        "# Предсказания на тестовой выборке\n",
        "y_pred_class = best_tree_classifier.predict(X_test_class)\n",
        "\n",
        "# Оценка качества\n",
        "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
        "f1 = f1_score(y_test_class, y_pred_class, average='weighted')\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test F1-Score: {f1:.4f}\")\n",
        "\n",
        "# 2. Обучение DecisionTreeRegressor с лучшими параметрами\n",
        "print(\"\\n=== Обучение Decision Tree Regressor с лучшими параметрами ===\")\n",
        "best_tree_regressor = DecisionTreeRegressor(\n",
        "    max_depth=best_params_regressor['max_depth'],\n",
        "    min_samples_split=best_params_regressor['min_samples_split'],\n",
        "    min_samples_leaf=best_params_regressor['min_samples_leaf'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "best_tree_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Предсказания на тестовой выборке\n",
        "y_pred_reg = best_tree_regressor.predict(X_test_reg)\n",
        "\n",
        "# Оценка качества\n",
        "rmse = root_mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n",
        "print(f\"Test R²: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj7ZWHTWAbyu",
        "outputId": "286feab3-2f82-4dbe-a6bf-93e3992b9e0b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Обучение Decision Tree Classifier с лучшими параметрами ===\n",
            "Test Accuracy: 0.8556\n",
            "Test F1-Score: 0.8590\n",
            "\n",
            "=== Обучение Decision Tree Regressor с лучшими параметрами ===\n",
            "Test RMSE: 6.6985\n",
            "Test R²: 0.8259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Итак, точность обоих моделей возросла, причем для регрессора - существенно (RMSE 6.69 против 9.57)"
      ],
      "metadata": {
        "id": "tYHvJp7IAo9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Имплементация алгоритма машинного обучения"
      ],
      "metadata": {
        "id": "9cXrKyD_A2DV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем собственную реализацию решающего дерева для классификации и регрессии, затем обучим модели на тестовых данных и сравним по качеству с реализациями из Sklearn."
      ],
      "metadata": {
        "id": "8Qv_lamrA7ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTreeNode:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature          # Признак для разбиения\n",
        "        self.threshold = threshold      # Пороговое значение\n",
        "        self.left = left                # Левое поддерево\n",
        "        self.right = right              # Правое поддерево\n",
        "        self.value = value              # Значение в листе (для терминальных узлов)\n",
        "\n",
        "class DecisionTreeClassifierCustom:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.root = None\n",
        "\n",
        "    def _gini(self, y):\n",
        "        proportions = np.bincount(y) / len(y)\n",
        "        return 1 - np.sum(proportions ** 2)\n",
        "\n",
        "    def _split(self, X, y, feature, threshold):\n",
        "        left_indices = np.where(X[:, feature] < threshold)[0]\n",
        "        right_indices = np.where(X[:, feature] >= threshold)[0]\n",
        "        return X[left_indices], y[left_indices], X[right_indices], y[right_indices]\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_score = float('inf')  # Лучший критерий разбиения (минимальный)\n",
        "        best_split = None\n",
        "\n",
        "        for feature in range(X.shape[1]):  # Перебор всех признаков\n",
        "            thresholds = np.unique(X[:, feature])  # Уникальные значения признака\n",
        "            for threshold in thresholds:\n",
        "                # Разбиение данных\n",
        "                X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n",
        "\n",
        "                # Проверка, что оба подмножества достаточно велики\n",
        "                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                # Вычисление критерия Gini\n",
        "                score = (len(y_left) * self._gini(y_left) + len(y_right) * self._gini(y_right)) / len(y)\n",
        "                if score < best_score:\n",
        "                    best_score = score\n",
        "                    best_split = (feature, threshold)\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        # Условие остановки\n",
        "        if (len(np.unique(y)) == 1 or depth == self.max_depth or\n",
        "            len(y) < self.min_samples_split):\n",
        "            return DecisionTreeNode(value=np.bincount(y).argmax())\n",
        "\n",
        "        # Найти лучшее разбиение\n",
        "        best_split = self._best_split(X, y)\n",
        "        if best_split is None:\n",
        "            return DecisionTreeNode(value=np.bincount(y).argmax())\n",
        "\n",
        "        feature, threshold = best_split\n",
        "\n",
        "        # Разделение данных\n",
        "        X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n",
        "        if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
        "            return DecisionTreeNode(value=np.bincount(y).argmax())\n",
        "\n",
        "        left_child = self._build_tree(X_left, y_left, depth + 1)\n",
        "        right_child = self._build_tree(X_right, y_right, depth + 1)\n",
        "\n",
        "        return DecisionTreeNode(feature, threshold, left_child, right_child)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y, 0)\n",
        "\n",
        "    def _predict(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature] < node.threshold:\n",
        "            return self._predict(x, node.left)\n",
        "        else:\n",
        "            return self._predict(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(x, self.root) for x in X])\n",
        "\n",
        "class DecisionTreeRegressorCustom:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.root = None\n",
        "\n",
        "    def _mse(self, y):\n",
        "        return np.mean((y - np.mean(y)) ** 2)\n",
        "\n",
        "    def _split(self, X, y, feature, threshold):\n",
        "        left_indices = np.where(X[:, feature] < threshold)[0]\n",
        "        right_indices = np.where(X[:, feature] >= threshold)[0]\n",
        "        return X[left_indices], y[left_indices], X[right_indices], y[right_indices]\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_score = float('inf')  # Лучший критерий разбиения (минимальный MSE)\n",
        "        best_split = None\n",
        "\n",
        "        for feature in range(X.shape[1]):  # Перебор всех признаков\n",
        "            thresholds = np.unique(X[:, feature])  # Уникальные значения признака\n",
        "            for threshold in thresholds:\n",
        "                # Разбиение данных\n",
        "                X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n",
        "\n",
        "                # Проверка, что оба подмножества достаточно велики\n",
        "                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                # Вычисление MSE\n",
        "                score = (len(y_left) * self._mse(y_left) + len(y_right) * self._mse(y_right)) / len(y)\n",
        "                if score < best_score:\n",
        "                    best_score = score\n",
        "                    best_split = (feature, threshold)\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        # Условие остановки\n",
        "        if (len(np.unique(y)) == 1 or depth == self.max_depth or\n",
        "            len(y) < self.min_samples_split):\n",
        "            return DecisionTreeNode(value=np.mean(y))\n",
        "\n",
        "        # Найти лучшее разбиение\n",
        "        best_split = self._best_split(X, y)\n",
        "        if best_split is None:\n",
        "            return DecisionTreeNode(value=np.mean(y))\n",
        "\n",
        "        feature, threshold = best_split\n",
        "\n",
        "        # Разделение данных\n",
        "        X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n",
        "        if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
        "            return DecisionTreeNode(value=np.mean(y))\n",
        "\n",
        "        left_child = self._build_tree(X_left, y_left, depth + 1)\n",
        "        right_child = self._build_tree(X_right, y_right, depth + 1)\n",
        "\n",
        "        return DecisionTreeNode(feature, threshold, left_child, right_child)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y, 0)\n",
        "\n",
        "    def _predict(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature] < node.threshold:\n",
        "            return self._predict(x, node.left)\n",
        "        else:\n",
        "            return self._predict(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(x, self.root) for x in X])"
      ],
      "metadata": {
        "id": "mUi_4dzeA1Br"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модели и оценим их качество"
      ],
      "metadata": {
        "id": "eT2yqlTCBakH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_class = scaler.fit_transform(X_train_class)\n",
        "X_test_class = scaler.transform(X_test_class)\n",
        "\n",
        "custom_tree_classifier = DecisionTreeClassifierCustom(max_depth=5, min_samples_split=2)\n",
        "custom_tree_classifier.fit(X_train_class, y_train_class)\n",
        "\n",
        "y_pred_class = custom_tree_classifier.predict(X_test_class)\n",
        "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
        "f1 = f1_score(y_test_class, y_pred_class, average='weighted')\n",
        "\n",
        "print(f\"Custom Decision Tree Classifier - Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnnT8yD6BZP3",
        "outputId": "a26fa4dc-1f14-4d7f-c89b-cbab83cbb71d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Decision Tree Classifier - Accuracy: 0.7889, F1-Score: 0.7776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_reg = scaler.fit_transform(X_train_reg)\n",
        "X_test_reg = scaler.transform(X_test_reg)\n",
        "\n",
        "custom_tree_regressor = DecisionTreeRegressorCustom(max_depth=5, min_samples_split=2)\n",
        "custom_tree_regressor.fit(X_train_reg, np.array(y_train_reg))\n",
        "\n",
        "y_pred_reg = custom_tree_regressor.predict(X_test_reg)\n",
        "rmse = root_mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"Custom Decision Tree Regressor - RMSE: {rmse:.4f}, R²: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYtpTJoNENXU",
        "outputId": "10b66eda-f18a-4891-cdce-ca59e6e3b1bb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Decision Tree Regressor - RMSE: 9.2196, R²: 0.6701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Точность реализованных вручную моделей в целом близка к библиотечным."
      ],
      "metadata": {
        "id": "wjIssh7dHxEG"
      }
    }
  ]
}