{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Лабораторная работа №5: Проведение исследований с градиентным бустингом"
      ],
      "metadata": {
        "id": "9VlZ9JhxWEFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Выбор начальных условий"
      ],
      "metadata": {
        "id": "aeNbl03UWFrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Был проведен в ЛР №1."
      ],
      "metadata": {
        "id": "n39vjl1fWHQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSWkFqeOV-pP",
        "outputId": "a9e02f25-08f4-42e0-ab0e-7cb4de46daa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub scikit-learn numpy pandas matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-lSUi0y5WOfW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачивание Date Fruit Dataset\n",
        "date_fruit_path = kagglehub.dataset_download(\"muratkokludataset/date-fruit-datasets\")\n",
        "\n",
        "# Чтение Excel-файла\n",
        "date_fruit_data = pd.read_excel(f\"{date_fruit_path}/Date_Fruit_Datasets/Date_Fruit_Datasets.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rHGTZGFWSGc",
        "outputId": "8b376486-f0fc-460d-a063-2996db17f6a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/muratkokludataset/date-fruit-datasets?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 408k/408k [00:00<00:00, 1.07MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачивание Concrete Compressive Strength\n",
        "concrete_strength_path = kagglehub.dataset_download(\"niteshyadav3103/concrete-compressive-strength\")\n",
        "\n",
        "# Чтение CSV-файла\n",
        "concrete_data = pd.read_csv(f\"{concrete_strength_path}/Concrete Compressive Strength.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkR2xBMbWU6P",
        "outputId": "5bd02dbd-9da0-4a6b-a30a-2def350dbcda"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/niteshyadav3103/concrete-compressive-strength?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14.1k/14.1k [00:00<00:00, 5.13MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Создание бейзлайна и оценка качества"
      ],
      "metadata": {
        "id": "c4IjNZ6jWY3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, root_mean_squared_error, r2_score, make_scorer\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "DVSuokOBWV26"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделим датасет для классификации на обучающую и тестовую выборки"
      ],
      "metadata": {
        "id": "0dvDZGJOWfzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение на признаки и целевую переменную\n",
        "X_class = date_fruit_data.drop(columns=['Class'])\n",
        "y_class = date_fruit_data['Class']\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
        "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "# Преобразование целевой переменной\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_class = label_encoder.fit_transform(y_train_class)\n",
        "y_test_class = label_encoder.transform(y_test_class)"
      ],
      "metadata": {
        "id": "a-yxIrqSWgs5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналогично разделим датасет для регрессии"
      ],
      "metadata": {
        "id": "6bfeqKOjX0rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение на признаки и целевую переменную\n",
        "X_reg = concrete_data.drop(columns=['Concrete compressive strength '])\n",
        "y_reg = concrete_data['Concrete compressive strength ']\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "OCPVnm6EX1Pi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модели для классификации и регрессии из Sklearn и оценим их"
      ],
      "metadata": {
        "id": "x2JT9UbsX2n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_classifier.fit(X_train_class, y_train_class)\n",
        "\n",
        "y_pred_class = gb_classifier.predict(X_test_class)\n",
        "\n",
        "accuracy_gb = accuracy_score(y_test_class, y_pred_class)\n",
        "f1_gb = f1_score(y_test_class, y_pred_class, average='weighted')\n",
        "\n",
        "print(f\"Gradient Boosting Classifier - Accuracy: {accuracy_gb:.4f}, F1-Score: {f1_gb:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEOMaxQyX4DH",
        "outputId": "56943dad-b8c6-4ed9-f760-57580bc306d3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Classifier - Accuracy: 0.8778, F1-Score: 0.8807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "y_pred_reg = gb_regressor.predict(X_test_reg)\n",
        "\n",
        "rmse_gb = root_mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2_gb = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"Gradient Boosting Regressor - RMSE: {rmse_gb:.4f}, R²: {r2_gb:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juR7gdoAYB2P",
        "outputId": "dd6e24fe-1ba5-42de-a5d1-bdeeb39e2134"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor - RMSE: 5.5422, R²: 0.8808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Итак, точность для встроенной в Sklearn модели градиентного бустинга для классификатора, получилась немного хуже, чем для случайного леса (87.8% точности), а для регрессора - на уровне случайного леса (5.54 RMSE). Попробуем улучшить бейзлайн."
      ],
      "metadata": {
        "id": "-rMb1nQtYOIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Улучшение бейзлайна"
      ],
      "metadata": {
        "id": "bYAm-lSKYq0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будем перебирать гиперпараметры градиентного бустинга, такие, как n_estimators, learning_rate, max_depth, min_samples_split, min_samples_leaf"
      ],
      "metadata": {
        "id": "14ZYAJ3yYwhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# === Подбор гиперпараметров для GradientBoostingClassifier ===\n",
        "print(\"=== Gradient Boosting Classifier - Hyperparameter Tuning ===\")\n",
        "param_dist_classifier = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "}\n",
        "\n",
        "random_search_gb_classifier = RandomizedSearchCV(\n",
        "    estimator=GradientBoostingClassifier(random_state=42),\n",
        "    param_distributions=param_dist_classifier,\n",
        "    n_iter=10,             # Ограничить количество случайных комбинаций\n",
        "    scoring='accuracy',    # Оптимизация Accuracy\n",
        "    cv=3,                  # Уменьшение количества фолдов\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_search_gb_classifier.fit(X_train_class, y_train_class)\n",
        "\n",
        "# Лучшие параметры и результат\n",
        "best_params_gb_classifier = random_search_gb_classifier.best_params_\n",
        "best_score_gb_classifier = random_search_gb_classifier.best_score_\n",
        "\n",
        "print(f\"Лучшие параметры для Gradient Boosting Classifier: {best_params_gb_classifier}\")\n",
        "print(f\"Лучший Accuracy на кросс-валидации: {best_score_gb_classifier:.4f}\")\n",
        "\n",
        "# === Подбор гиперпараметров для GradientBoostingRegressor ===\n",
        "print(\"\\n=== Gradient Boosting Regressor - Hyperparameter Tuning ===\")\n",
        "param_dist_regressor = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "}\n",
        "\n",
        "random_search_gb_regressor = RandomizedSearchCV(\n",
        "    estimator=GradientBoostingRegressor(random_state=42),\n",
        "    param_distributions=param_dist_regressor,\n",
        "    n_iter=10,             # Ограничить количество случайных комбинаций\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_search_gb_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Лучшие параметры и результат\n",
        "best_params_gb_regressor = random_search_gb_regressor.best_params_\n",
        "best_score_gb_regressor = -random_search_gb_regressor.best_score_\n",
        "\n",
        "print(f\"Лучшие параметры для Gradient Boosting Regressor: {best_params_gb_regressor}\")\n",
        "print(f\"Лучший RMSE на кросс-валидации: {best_score_gb_regressor:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am2aIScEZB1-",
        "outputId": "4ba3d218-f910-4264-ac26-0074d54b0c9b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Gradient Boosting Classifier - Hyperparameter Tuning ===\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Лучшие параметры для Gradient Boosting Classifier: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 3, 'learning_rate': 0.1}\n",
            "Лучший Accuracy на кросс-валидации: 0.8593\n",
            "\n",
            "=== Gradient Boosting Regressor - Hyperparameter Tuning ===\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Лучшие параметры для Gradient Boosting Regressor: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 5, 'learning_rate': 0.2}\n",
            "Лучший RMSE на кросс-валидации: 4.7705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модели с лучшими гиперпараметрами"
      ],
      "metadata": {
        "id": "z51vJIEJcrYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GradientBoostingClassifier с подобранными параметрами\n",
        "best_gb_classifier = GradientBoostingClassifier(\n",
        "    n_estimators=best_params_gb_classifier['n_estimators'],\n",
        "    learning_rate=best_params_gb_classifier['learning_rate'],\n",
        "    max_depth=best_params_gb_classifier['max_depth'],\n",
        "    min_samples_split=best_params_gb_classifier['min_samples_split'],\n",
        "    min_samples_leaf=best_params_gb_classifier['min_samples_leaf'],\n",
        "    random_state=42\n",
        ")\n",
        "best_gb_classifier.fit(X_train_class, y_train_class)\n",
        "y_pred_class = best_gb_classifier.predict(X_test_class)\n",
        "\n",
        "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
        "f1 = f1_score(y_test_class, y_pred_class, average='weighted')\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test F1-Score: {f1:.4f}\")\n",
        "\n",
        "# GradientBoostingRegressor с подобранными параметрами\n",
        "best_gb_regressor = GradientBoostingRegressor(\n",
        "    n_estimators=best_params_gb_regressor['n_estimators'],\n",
        "    learning_rate=best_params_gb_regressor['learning_rate'],\n",
        "    max_depth=best_params_gb_regressor['max_depth'],\n",
        "    min_samples_split=best_params_gb_regressor['min_samples_split'],\n",
        "    min_samples_leaf=best_params_gb_regressor['min_samples_leaf'],\n",
        "    random_state=42\n",
        ")\n",
        "best_gb_regressor.fit(X_train_reg, y_train_reg)\n",
        "y_pred_reg = best_gb_regressor.predict(X_test_reg)\n",
        "\n",
        "rmse = root_mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n",
        "print(f\"Test R²: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KUYwoVvcpoQ",
        "outputId": "82b79bfc-73e5-4e3d-924c-954d878b0d35"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8833\n",
            "Test F1-Score: 0.8868\n",
            "Test RMSE: 4.3941\n",
            "Test R²: 0.9251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Точность для классификации выросла ненамного, а вот для регрессии, кажется, это лучший результат среди всех рассмотренных моделей в работах."
      ],
      "metadata": {
        "id": "637JuO-Mc89y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Имплементация алгоритма машинного обучения"
      ],
      "metadata": {
        "id": "NScYDvqBdOLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем собственную реализацию градиентного бустинга для классификации и регрессии, затем обучим модели на тестовых данных и сравним по качеству с реализациями из Sklearn. Будем использовать реализацию решающего дерева из ЛР №3."
      ],
      "metadata": {
        "id": "7j1LoCl1dQFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class DecisionTreeNode:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature          # Признак для разбиения\n",
        "        self.threshold = threshold      # Пороговое значение\n",
        "        self.left = left                # Левое поддерево\n",
        "        self.right = right              # Правое поддерево\n",
        "        self.value = value              # Значение в листе (для терминальных узлов)\n",
        "\n",
        "class DecisionTreeRegressorCustom:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.root = None\n",
        "\n",
        "    def _mse(self, y):\n",
        "        return np.mean((y - np.mean(y)) ** 2)\n",
        "\n",
        "    def _split(self, X, y, feature, threshold):\n",
        "        left_indices = np.where(X[:, feature] < threshold)[0]\n",
        "        right_indices = np.where(X[:, feature] >= threshold)[0]\n",
        "        return X[left_indices], y[left_indices], X[right_indices], y[right_indices]\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_score = float('inf')  # Лучший критерий разбиения (минимальный MSE)\n",
        "        best_split = None\n",
        "\n",
        "        for feature in range(X.shape[1]):  # Перебор всех признаков\n",
        "            thresholds = np.unique(X[:, feature])  # Уникальные значения признака\n",
        "            for threshold in thresholds:\n",
        "                # Разбиение данных\n",
        "                X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n",
        "\n",
        "                # Проверка, что оба подмножества достаточно велики\n",
        "                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                # Вычисление MSE\n",
        "                score = (len(y_left) * self._mse(y_left) + len(y_right) * self._mse(y_right)) / len(y)\n",
        "                if score < best_score:\n",
        "                    best_score = score\n",
        "                    best_split = (feature, threshold)\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        # Условие остановки\n",
        "        if (len(np.unique(y)) == 1 or depth == self.max_depth or\n",
        "            len(y) < self.min_samples_split):\n",
        "            return DecisionTreeNode(value=np.mean(y))\n",
        "\n",
        "        # Найти лучшее разбиение\n",
        "        best_split = self._best_split(X, y)\n",
        "        if best_split is None:\n",
        "            return DecisionTreeNode(value=np.mean(y))\n",
        "\n",
        "        feature, threshold = best_split\n",
        "\n",
        "        # Разделение данных\n",
        "        X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n",
        "        if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
        "            return DecisionTreeNode(value=np.mean(y))\n",
        "\n",
        "        left_child = self._build_tree(X_left, y_left, depth + 1)\n",
        "        right_child = self._build_tree(X_right, y_right, depth + 1)\n",
        "\n",
        "        return DecisionTreeNode(feature, threshold, left_child, right_child)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y, 0)\n",
        "\n",
        "    def _predict(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature] < node.threshold:\n",
        "            return self._predict(x, node.left)\n",
        "        else:\n",
        "            return self._predict(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(x, self.root) for x in X])\n",
        "\n",
        "class CustomGradientBoostingClassifier:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "        self.init_pred = None\n",
        "\n",
        "    def _log_loss_gradient(self, y_true, y_pred_proba):\n",
        "        n_classes = y_pred_proba.shape[1]\n",
        "        y_one_hot = np.zeros_like(y_pred_proba)\n",
        "        y_one_hot[np.arange(len(y_true)), y_true] = 1  # One-hot encoding меток\n",
        "        return y_pred_proba - y_one_hot\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_classes = len(np.unique(y))\n",
        "        # Инициализация начальных прогнозов log-odds\n",
        "        class_counts = np.bincount(y, minlength=n_classes)\n",
        "        self.init_pred = np.log(class_counts / len(y) + 1e-8)  # Добавляем маленькое значение для численной стабильности\n",
        "        y_pred_proba = np.tile(self.init_pred, (len(y), 1))  # Матрица начальных прогнозов\n",
        "        self.models = []\n",
        "\n",
        "        for _ in tqdm(range(self.n_estimators), desc=\"Training Gradient Boosting Classifier\"):\n",
        "            # Преобразование текущих прогнозов в вероятности с использованием softmax\n",
        "            probs = np.exp(y_pred_proba) / np.sum(np.exp(y_pred_proba), axis=1, keepdims=True)\n",
        "\n",
        "            # Вычисление остатка (градиента логистической функции потерь)\n",
        "            residuals = self._log_loss_gradient(y, probs)\n",
        "\n",
        "            trees = []\n",
        "            for c in range(n_classes):\n",
        "                # Обучение дерева на остатках для текущего класса\n",
        "                tree = DecisionTreeRegressorCustom(max_depth=self.max_depth)\n",
        "                tree.fit(X, residuals[:, c])\n",
        "                trees.append(tree)\n",
        "\n",
        "            self.models.append(trees)\n",
        "\n",
        "            # Обновление прогнозов\n",
        "            for c, tree in enumerate(trees):\n",
        "                y_pred_proba[:, c] += self.learning_rate * tree.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_classes = len(self.init_pred)\n",
        "        pred_proba = np.tile(self.init_pred, (X.shape[0], 1))  # Матрица начальных прогнозов\n",
        "        for trees in self.models:\n",
        "            for c, tree in enumerate(trees):\n",
        "                pred_proba[:, c] += self.learning_rate * tree.predict(X)\n",
        "        probs = np.exp(pred_proba) / np.sum(np.exp(pred_proba), axis=1, keepdims=True)  # Softmax для вероятностей\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "class CustomGradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "        self.init_pred = None\n",
        "\n",
        "    def _mse_gradient(self, y_true, y_pred):\n",
        "        return y_true - y_pred\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.init_pred = np.mean(y)  # Начальный прогноз\n",
        "        y_pred = np.ones(len(y)) * self.init_pred\n",
        "        self.models = []\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = self._mse_gradient(y, y_pred)\n",
        "            tree = DecisionTreeRegressorCustom(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            self.models.append(tree)\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.ones(X.shape[0]) * self.init_pred\n",
        "        for tree in self.models:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "WWlpXgTPdiHX"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модели и оценим их качество"
      ],
      "metadata": {
        "id": "Fz84yTlxdsQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_gb_classifier = CustomGradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=3)\n",
        "custom_gb_classifier.fit(X_train_class, y_train_class)\n",
        "\n",
        "y_pred_custom_class = custom_gb_classifier.predict(X_test_class)\n",
        "\n",
        "accuracy_custom = accuracy_score(y_test_class, y_pred_custom_class)\n",
        "f1_custom = f1_score(y_test_class, y_pred_custom_class, average='weighted')\n",
        "\n",
        "print(f\"Custom Gradient Boosting Classifier - Accuracy: {accuracy_custom:.4f}, F1-Score: {f1_custom:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOCH_BeSdu3D",
        "outputId": "af9db6f1-4364-4451-8221-c32e24b87b0e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Gradient Boosting Classifier: 100%|██████████| 10/10 [06:32<00:00, 39.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Gradient Boosting Classifier - Accuracy: 0.0056, F1-Score: 0.0026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_reg = scaler.fit_transform(X_train_reg)\n",
        "X_test_reg = scaler.transform(X_test_reg)\n",
        "\n",
        "custom_gb_regressor = CustomGradientBoostingRegressor(n_estimators=10, learning_rate=0.1, max_depth=3)\n",
        "custom_gb_regressor.fit(X_train_reg, np.array(y_train_reg))\n",
        "\n",
        "y_pred_custom_reg = custom_gb_regressor.predict(X_test_reg)\n",
        "\n",
        "rmse_custom = root_mean_squared_error(y_test_reg, y_pred_custom_reg)\n",
        "r2_custom = r2_score(y_test_reg, y_pred_custom_reg)\n",
        "\n",
        "print(f\"Custom Gradient Boosting Regressor - RMSE: {rmse_custom:.4f}, R²: {r2_custom:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqo6opDAibRu",
        "outputId": "a1e28d87-93bd-4d8b-b5d1-26dc938f194f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Gradient Boosting Regressor - RMSE: 10.2411, R²: 0.5930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель классификации работала 6.5 минут и показала ужасные результаты. Модель регрессии отработала сравнительно быстро и показала средние, но приемлимые результаты."
      ],
      "metadata": {
        "id": "wMMpZ84anRpo"
      }
    }
  ]
}